{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 game_player 文件夹复制到当前文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前目录结构\n",
    "\n",
    "- pysekiro_with_RL\n",
    "    - game_player\n",
    "        - \\__init__.py\n",
    "        - brain.py\n",
    "        - control_keyboard_keys.py\n",
    "        - detect_keyboard_keys.py\n",
    "        - grab_screen.py\n",
    "        - others.py\n",
    "        - run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 读取游戏画面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码：[grab_screen.py](https://github.com/ricagj/train_your_own_game_AI/blob/main/game_player/grab_screen.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调节游戏的分辨率，最后选最小的，越小越好，我这里选择的是 800x450 的。（**修改分辨率后最好重启游戏**）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**强烈建议把游戏放左上角**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行以下代码，找到边框位置\n",
    "from game_player.grab_screen import get_full_screen\n",
    "from game_player.others import get_xywh\n",
    "screen = get_full_screen()\n",
    "get_xywh(screen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开 grab_screen.py ,修改如下  \n",
    "**修改前**\n",
    "~~~python\n",
    "# ---------- 注意：以下需要设置 ----------\n",
    "\n",
    "GAME_WIDTH   = 0    # 游戏窗口宽度\n",
    "GAME_HEIGHT  = 0    # 游戏窗口高度\n",
    "white_border = 0    # 游戏边框\n",
    "\n",
    "# ---------- 注意：以上需要设置 ----------\n",
    "~~~\n",
    "**修改后**\n",
    "~~~python\n",
    "# ---------- 注意：以下需要设置 ----------\n",
    "\n",
    "GAME_WIDTH   = 800    # 游戏窗口宽度，按游戏里面的来填\n",
    "GAME_HEIGHT  = 450    # 游戏窗口高度，按游戏里面的来填\n",
    "white_border = 31     # 游戏边框，把上面生成的y值填入这里\n",
    "\n",
    "# ---------- 注意：以上需要设置 ----------\n",
    "~~~\n",
    "保存 grab_screen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**记得重启内核，否则或报错**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试效果\n",
    "import cv2\n",
    "from game_player.grab_screen import get_game_screen\n",
    "\n",
    "screen = get_game_screen()\n",
    "cv2.imshow('screen', screen)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 操作游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码：[control_keyboard_keys.py](https://github.com/ricagj/train_your_own_game_AI/blob/main/game_player/control_keyboard_keys.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据游戏里设置的按键来定义执行相应动作函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我设置的是攻击、弹反、垫步、跳跃，位移太复杂就不要了  \n",
    "- 攻击键我设置为 **J**\n",
    "- 弹反键我设置为 **K**\n",
    "- 垫步键默认是 左边的SHIFT键，在第57~166行内的第145行发现 **LSHIFT**，代表左SHIFT键\n",
    "- 跳跃键默认是 空格键，在第57~166行内的第133行发现 **SPACE**，代表空格键\n",
    "\n",
    "delay我设置的是0.05 ，因为按这些键不需要很长时间，我做过实验，发现0.01秒也是可行的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开 control_keyboard_keys.py ,修改如下  \n",
    "**修改前**\n",
    "~~~python\n",
    "# ---------- 以下需要修改或补充 ----------\n",
    "\n",
    "def W(delay=1):    # 移动 前\n",
    "    PressKey(dk['W'])\n",
    "    time.sleep(delay)\n",
    "    ReleaseKey(dk['W'])\n",
    "\n",
    "def S(delay=1):    # 移动 后\n",
    "    PressKey(dk['S'])\n",
    "    time.sleep(delay)\n",
    "    ReleaseKey(dk['S'])\n",
    "\n",
    "def A(delay=1):    # 移动 左\n",
    "    PressKey(dk['A'])\n",
    "    time.sleep(delay)\n",
    "    ReleaseKey(dk['A'])\n",
    "\n",
    "def D(delay=1):    # 移动 右\n",
    "    PressKey(dk['D'])\n",
    "    time.sleep(delay)\n",
    "    ReleaseKey(dk['D'])\n",
    "\n",
    "# ---------- 以上需要修改或补充 ----------\n",
    "~~~\n",
    "**修改后**\n",
    "~~~python\n",
    "# ---------- 以下需要修改或补充 ----------\n",
    "\n",
    "def J(delay=0.05):    # 攻击\n",
    "    PressKey(dk['J'])\n",
    "    time.sleep(delay)\n",
    "    ReleaseKey(dk['J'])\n",
    "\n",
    "def K(delay=0.05):    # 防御\n",
    "    PressKey(dk['K'])\n",
    "    time.sleep(delay)\n",
    "    ReleaseKey(dk['K'])\n",
    "\n",
    "def LSHIFT(delay=0.05):    # 垫步\n",
    "    PressKey(dk['LSHIFT'])\n",
    "    time.sleep(delay)\n",
    "    ReleaseKey(dk['LSHIFT'])\n",
    "\n",
    "def SPACE(delay=0.05):    # 跳跃\n",
    "    PressKey(dk['SPACE'])\n",
    "    time.sleep(delay)\n",
    "    ReleaseKey(dk['SPACE'])\n",
    "\n",
    "# ---------- 以上需要修改或补充 ----------\n",
    "~~~\n",
    "保存 control_keyboard_keys.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**记得重启内核，否则或报错**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试一下效果\n",
    "import time\n",
    "\n",
    "from game_player.control_keyboard_keys import J, K, LSHIFT, SPACE\n",
    "\n",
    "time.sleep(3)\n",
    "for _ in range(5):\n",
    "    print(1)\n",
    "    J()\n",
    "    time.sleep(0.7)\n",
    "    K()\n",
    "    time.sleep(0.2)\n",
    "    LSHIFT()\n",
    "    time.sleep(0.7)\n",
    "    SPACE()\n",
    "    time.sleep(1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 修改智能体的动作执行部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码：[brain.py](https://github.com/ricagj/train_your_own_game_AI/blob/main/game_player/brain.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开 brain.py ,修改如下\n",
    "\n",
    "第一部分 **修改前**\n",
    "~~~python\n",
    "# ---------- 以下根据 control_keyboard_keys.py 里定义的函数来导入 ----------\n",
    "from game_player.control_keyboard_keys import W, S, A, D\n",
    "# ---------- 以上根据 control_keyboard_keys.py 里定义的函数来导入 ----------\n",
    "~~~\n",
    "第一部分 **修改后**\n",
    "~~~python\n",
    "# ---------- 以下根据 control_keyboard_keys.py 里定义的函数来导入 ----------\n",
    "from game_player.control_keyboard_keys import J, K, LSHIFT, SPACE\n",
    "# ---------- 以上根据 control_keyboard_keys.py 里定义的函数来导入 ----------\n",
    "~~~\n",
    "\n",
    "第二部分 **修改前**\n",
    "~~~python\n",
    "# ---------- 以下根据 control_keyboard_keys.py 里定义的函数来修改 ----------\n",
    "\n",
    "\"\"\"\n",
    "将所有的动作都编码成数字，并且数字满足从零开始和正整数的要求。\n",
    "例如\n",
    "    W 移动 前 0\n",
    "    S 移动 后 1\n",
    "    A 移动 左 2\n",
    "    D 移动 右 3\n",
    "\"\"\"\n",
    "\n",
    "# 执行动作\n",
    "if   action == 0:\n",
    "    W()\n",
    "elif action == 1:\n",
    "    S()\n",
    "elif action == 2:\n",
    "    A()\n",
    "elif action == 3:\n",
    "    D()\n",
    "elif action == 4:    # 等你添加，不需要可以删除\n",
    "    pass\n",
    "# 不够可以添加，注意，一定要是正整数，还要和上一个相邻\n",
    "# ---------- 以上根据 control_keyboard_keys.py 里定义的函数来修改 ----------\n",
    "~~~\n",
    "第二部分 **修改后**\n",
    "~~~python\n",
    "# ---------- 以下根据 control_keyboard_keys.py 里定义的函数来修改 ----------\n",
    "\n",
    "\"\"\"\n",
    "将所有的动作都编码成数字，并且数字满足从零开始和正整数的要求。\n",
    "例如\n",
    "    J      攻击 0\n",
    "    K      弹反 1\n",
    "    LSHIFT 垫步 2\n",
    "    SPACE  跳跃 3\n",
    "\"\"\"\n",
    "\n",
    "# 执行动作\n",
    "if   action == 0:\n",
    "    J()\n",
    "elif action == 1:\n",
    "    K()\n",
    "elif action == 2:\n",
    "    LSHIFT()\n",
    "elif action == 3:\n",
    "    SPACE()\n",
    "\n",
    "# 不够可以添加，注意，一定要是正整数，还要和上一个相邻\n",
    "# ---------- 以上根据 control_keyboard_keys.py 里定义的函数来修改 ----------\n",
    "~~~\n",
    "\n",
    "保存 brain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 量化人物状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码：[others.py](https://github.com/ricagj/train_your_own_game_AI/blob/main/game_player/others.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：本节采用边缘检测的方法量化人物状态，但是这个方法有一个缺陷，就是必须边缘存在，否则无法正确量化为数值。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建议在游戏里截一张屏设为壁纸，就不用总是开始游戏了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 读取自身生命"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取游戏画面的代码\n",
    "import cv2\n",
    "from game_player.grab_screen import get_game_screen\n",
    "\n",
    "screen = get_game_screen()\n",
    "cv2.imshow('screen', screen)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_player.others import get_xywh\n",
    "get_xywh(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_player.others import roi\n",
    "screen_roi = roi(screen, x=48, x_w=307, y=406, y_h=410)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canny = cv2.Canny(cv2.GaussianBlur(screen_roi,(3,3),0), 0, 100)\n",
    "value = canny.argmax(axis=-1)\n",
    "print(value)\n",
    "print('平均值', np.mean(value))\n",
    "print('中位数', np.median(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Self_HP(img):\n",
    "    img = roi(img, x=48, x_w=307, y=406, y_h=410)\n",
    "    canny = cv2.Canny(cv2.GaussianBlur(img,(3,3),0), 0, 100)\n",
    "    value = canny.argmax(axis=-1)\n",
    "    return np.median(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试效果\n",
    "while True:\n",
    "    screen = get_game_screen()\n",
    "    Self_HP = get_Self_HP(screen)\n",
    "    print(f'\\r {str(Self_HP):<10}', end='')\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 读取自身架势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**受架势显示与否的影响，不显示就无法正确读取**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取游戏画面的代码\n",
    "import cv2\n",
    "from game_player.grab_screen import get_game_screen\n",
    "\n",
    "screen = get_game_screen()\n",
    "cv2.imshow('screen', screen)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_player.others import get_xywh\n",
    "get_xywh(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_player.others import roi\n",
    "screen_roi = roi(screen, x=402, x_w=491, y=388, y_h=390)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canny = cv2.Canny(cv2.GaussianBlur(screen_roi,(3,3),0), 0, 100)\n",
    "value = canny.argmax(axis=-1)\n",
    "print(value)\n",
    "print('平均值', np.mean(value))\n",
    "print('中位数', np.median(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Self_Posture(img):\n",
    "    img = roi(img, x=402, x_w=491, y=388, y_h=390)\n",
    "    canny = cv2.Canny(cv2.GaussianBlur(img,(3,3),0), 0, 100)\n",
    "    value = canny.argmax(axis=-1)\n",
    "    return np.median(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试效果\n",
    "while True:\n",
    "    screen = get_game_screen()\n",
    "    Self_Posture = get_Self_Posture(screen)\n",
    "    print(f'\\r {str(Self_Posture):<10}', end='')\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 读取目标生命"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取游戏画面的代码\n",
    "import cv2\n",
    "from game_player.grab_screen import get_game_screen\n",
    "\n",
    "screen = get_game_screen()\n",
    "cv2.imshow('screen', screen)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_player.others import get_xywh\n",
    "get_xywh(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_player.others import roi\n",
    "screen_roi = roi(screen, x=48, x_w=219, y=40, y_h=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canny = cv2.Canny(cv2.GaussianBlur(screen_roi,(3,3),0), 0, 100)\n",
    "value = canny.argmax(axis=-1)\n",
    "print(value)\n",
    "print('平均值', np.mean(value))\n",
    "print('中位数', np.median(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Target_HP(img):\n",
    "    img = roi(img, x=48, x_w=219, y=40, y_h=45)\n",
    "    canny = cv2.Canny(cv2.GaussianBlur(img,(3,3),0), 0, 100)\n",
    "    value = canny.argmax(axis=-1)\n",
    "    return np.median(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试效果\n",
    "while True:\n",
    "    screen = get_game_screen()\n",
    "    Target_HP = get_Target_HP(screen)\n",
    "    print(f'\\r {str(Target_HP):<10}', end='')\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 读取目标架势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**受架势显示与否的影响，不显示就无法正确读取**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取游戏画面的代码\n",
    "import cv2\n",
    "from game_player.grab_screen import get_game_screen\n",
    "\n",
    "screen = get_game_screen()\n",
    "cv2.imshow('screen', screen)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_player.others import get_xywh\n",
    "get_xywh(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_player.others import roi\n",
    "screen_roi = roi(screen, x=402, x_w=554, y=27, y_h=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canny = cv2.Canny(cv2.GaussianBlur(screen_roi,(3,3),0), 0, 100)\n",
    "value = canny.argmax(axis=-1)\n",
    "print(value)\n",
    "print('平均值', np.mean(value))\n",
    "print('中位数', np.median(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Target_Posture(img):\n",
    "    img = roi(img, x=402, x_w=554, y=27, y_h=31)\n",
    "    canny = cv2.Canny(cv2.GaussianBlur(img,(3,3),0), 0, 100)\n",
    "    value = canny.argmax(axis=-1)\n",
    "    return np.median(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试效果\n",
    "while True:\n",
    "    screen = get_game_screen()\n",
    "    Target_Posture = get_Target_Posture(screen)\n",
    "    print(f'\\r {str(Target_Posture):<10}', end='')\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 整合数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status(img):\n",
    "    return get_Self_HP(img), get_Self_Posture(img), get_Target_HP(img), get_Target_Posture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开 others.py ,修改如下  \n",
    "**修改前**\n",
    "~~~python\n",
    "# ---------- 以下需要修改 ----------\n",
    "\n",
    "def get_state_1(img):    # 自己改\n",
    "    return 0\n",
    "\n",
    "def get_state_2(img):    # 自己改\n",
    "    return 0\n",
    "\n",
    "def get_state_3(img):    # 自己改\n",
    "    return 0\n",
    "\n",
    "def get_state_4(img):    # 自己改\n",
    "    return 0\n",
    "\n",
    "# 不够就自己添加，多了就自己删除\n",
    "\n",
    "def get_status(img):\n",
    "    return get_state_1(img), get_state_2(img), get_state_3(img), get_state_4(img)    # 这里也要改成相应的函数名\n",
    "\n",
    "# ---------- 以上需要修改 ----------\n",
    "~~~\n",
    "**修改后**\n",
    "~~~python\n",
    "# ---------- 以下需要修改 ----------\n",
    "\n",
    "def get_Self_HP(img):\n",
    "    img = roi(img, x=48, x_w=307, y=406, y_h=410)\n",
    "    canny = cv2.Canny(cv2.GaussianBlur(img,(3,3),0), 0, 100)\n",
    "    value = canny.argmax(axis=-1)\n",
    "    return np.median(value)\n",
    "\n",
    "def get_Self_Posture(img):\n",
    "    img = roi(img, x=402, x_w=491, y=388, y_h=390)\n",
    "    canny = cv2.Canny(cv2.GaussianBlur(img,(3,3),0), 0, 100)\n",
    "    value = canny.argmax(axis=-1)\n",
    "    return np.median(value)\n",
    "\n",
    "def get_Target_HP(img):\n",
    "    img = roi(img, x=48, x_w=219, y=40, y_h=45)\n",
    "    canny = cv2.Canny(cv2.GaussianBlur(img,(3,3),0), 0, 100)\n",
    "    value = canny.argmax(axis=-1)\n",
    "    return np.median(value)\n",
    "\n",
    "def get_Target_Posture(img):\n",
    "    img = roi(img, x=402, x_w=554, y=27, y_h=31)\n",
    "    canny = cv2.Canny(cv2.GaussianBlur(img,(3,3),0), 0, 100)\n",
    "    value = canny.argmax(axis=-1)\n",
    "    return np.median(value)\n",
    "\n",
    "# 不够就自己添加，多了就自己删除\n",
    "\n",
    "def get_status(img):\n",
    "    return get_Self_HP(img), get_Self_Posture(img), get_Target_HP(img), get_Target_Posture(img)\n",
    "\n",
    "# ---------- 以上需要修改 ----------\n",
    "~~~\n",
    "保存 others.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试效果\n",
    "import cv2\n",
    "from game_player.grab_screen import get_game_screen\n",
    "from game_player.others import get_status\n",
    "\n",
    "while True:\n",
    "    screen = get_game_screen()\n",
    "    status = get_status(screen)\n",
    "    print(f'\\r {str(status):<30}', end='')\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 修改模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码：[brain.py](https://github.com/ricagj/train_your_own_game_AI/blob/main/game_player/brain.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：这个只是参考示例，由于我电脑太菜了所以这样设置的，请根据自己电脑的实际情况再做调整。我这个模型的配置，八成是没什么学习能力的，所以用这个配置还有这个结构来训练的话到时没训练出效果别来找我。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开 brain.py ,修改如下  \n",
    "**修改前**\n",
    "~~~python\n",
    "# -------------------- 以下务必自己修改 --------------------\n",
    "# 第 1 层 卷积层和最大池化层\n",
    "conv_1 = tf.keras.layers.Conv2D(filters=1, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)(Input)\n",
    "pool_1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(conv_1)\n",
    "\n",
    "# 第 2 层 卷积层和最大池化层\n",
    "conv_2 = tf.keras.layers.Conv2D(filters=1, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)(pool_1)\n",
    "pool_2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(conv_2)\n",
    "\n",
    "# 你要是觉得不够，可以自己增加卷积层和池化层，觉得太多了就删掉\n",
    "\n",
    "# 扁平化层\n",
    "flat = tf.keras.layers.Flatten()(pool_2)\n",
    "\n",
    "# 第 1 层 全连接层\n",
    "dense_1 = tf.keras.layers.Dense(1, activation=tf.nn.relu)(flat)\n",
    "dense_1 = tf.keras.layers.BatchNormalization()(dense_1)\n",
    "\n",
    "# 第 2 层 全连接层\n",
    "dense_2 = tf.keras.layers.Dense(1, activation=tf.nn.relu)(dense_1)\n",
    "dense_2 = tf.keras.layers.BatchNormalization()(dense_2)\n",
    "\n",
    "output = dense_2\n",
    "# -------------------- 以上务必自己修改 --------------------\n",
    "~~~\n",
    "**修改后**\n",
    "~~~python\n",
    "# -------------------- 以下务必自己修改 --------------------\n",
    "# 第 1 层 卷积层和最大池化层\n",
    "conv_1 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)(Input)\n",
    "pool_1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(conv_1)\n",
    "\n",
    "# 第 2 层 卷积层和最大池化层\n",
    "conv_2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation=tf.nn.relu)(pool_1)\n",
    "pool_2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(conv_2)\n",
    "\n",
    "# 你要是觉得不够，可以自己增加卷积层和池化层，觉得太多了就删掉\n",
    "\n",
    "# 扁平化层\n",
    "flat = tf.keras.layers.Flatten()(pool_2)\n",
    "\n",
    "# 第 1 层 全连接层\n",
    "dense_1 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(flat)\n",
    "dense_1 = tf.keras.layers.BatchNormalization()(dense_1)\n",
    "\n",
    "# 第 2 层 全连接层\n",
    "dense_2 = tf.keras.layers.Dense(16, activation=tf.nn.relu)(dense_1)\n",
    "dense_2 = tf.keras.layers.BatchNormalization()(dense_2)\n",
    "\n",
    "output = dense_2\n",
    "# -------------------- 以上务必自己修改 --------------------\n",
    "~~~\n",
    "保存 brain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 设置奖励"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码：[run.py](https://github.com/ricagj/train_your_own_game_AI/blob/main/game_player/run.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 整合数据时定义的状态获取**\n",
    "~~~python\n",
    "def get_status(img):\n",
    "    return get_Self_HP(img), get_Self_Posture(img), get_Target_HP(img), get_Target_Posture\n",
    "~~~\n",
    "返回的是一个元组，(自身生命值，自身架势值，目标生命值，目标架势值)，因此\n",
    "~~~python\n",
    "\"\"\"\n",
    "next_status - cur_status\n",
    "等于 (自身生命变化值，自身架势变化值，目标生命变化值，目标架势变化值)\n",
    "\"\"\"\n",
    "s1 = next_status[0] - cur_status[0]    # 自身生命变化值\n",
    "s2 = next_status[1] - cur_status[1]    # 自身架势变化值\n",
    "s3 = next_status[2] - cur_status[2]    # 目标生命变化值\n",
    "s4 = next_status[3] - cur_status[3]    # 目标架势变化值\n",
    "~~~\n",
    "**注意：未来 - 现在**  \n",
    "**注意：未来 - 现在**  \n",
    "**注意：未来 - 现在**  \n",
    "- 所以\n",
    "    - 自身生命变化值 与 奖励 呈**正**相关\n",
    "    - 自身架势变化值 与 奖励 呈**负**相关\n",
    "    - 目标生命变化值 与 奖励 呈**负**相关\n",
    "    - 目标架势变化值 与 奖励 呈**正**相关\n",
    "\n",
    "~~~python\n",
    "s1 *=  1    # 正相关\n",
    "s2 *= -1    # 负相关\n",
    "s3 *= -1    # 负相关\n",
    "s4 *=  1    # 正相关\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开 run.py ,修改如下  \n",
    "**修改前**\n",
    "~~~python\n",
    "# ---------- 以下根据 others.py 里定义的函数 get_status 来修改 ----------\n",
    "# 通过列表索引的方式，取出相应的信息，用未来的状态信息减去当前的状态信息，得到状态变化值\n",
    "s1 = next_status[0] - cur_status[0]\n",
    "s2 = next_status[1] - cur_status[1]\n",
    "s3 = next_status[2] - cur_status[2]\n",
    "s4 = next_status[3] - cur_status[3]\n",
    "\n",
    "\"\"\"\n",
    "注意，未来 - 现在\n",
    "假如你现在生命值 130（现在），过了一会生命值变成 63（未来）\n",
    "计算：s = 63 - 130, s = -67, 生命值降低了67，生命值减低应该惩罚，那么s完全可以当成得分，得到 -67 分。\n",
    "\n",
    "再假如Boss 现在生命值 112（现在）， 过了一会生命值变成 102（未来）\n",
    "计算：s = 102 - 112, s = -10, Boss生命值降低了10，应该奖励才对，但是s为负值，所以要乘上 -1 ，这样才能得到正常的分数。\n",
    "\n",
    "请根据具体的游戏来定义，不要生搬硬套，别搞得HP掉了还加分\n",
    "\"\"\"\n",
    "# 示例 定义得分\n",
    "s1 *=  1    # 与 奖励 呈正相关，所以 +\n",
    "s2 *= -1    # 与 惩罚 呈正相关，所以 -\n",
    "s3 *= -1    # 与 惩罚 呈正相关，所以 -\n",
    "s4 *=  1    # 与 奖励 呈正相关，所以 +\n",
    "\n",
    "reward = s1 + s2 + s3 +s4\n",
    "# ---------- 以上根据 others.py 里定义的函数 get_status 来修改 ----------\n",
    "~~~\n",
    "**修改后**\n",
    "~~~python\n",
    "# ---------- 以下根据 others.py 里定义的函数 get_status 来修改 ----------\n",
    "# 通过列表索引的方式，取出相应的信息，用未来的状态信息减去当前的状态信息，得到状态变化值\n",
    "\"\"\"\n",
    "next_status - cur_status\n",
    "等于 (自身生命变化值，自身架势变化值，目标生命变化值，目标架势变化值)\n",
    "\"\"\"\n",
    "s1 = next_status[0] - cur_status[0]    # 自身生命变化值\n",
    "s2 = next_status[1] - cur_status[1]    # 自身架势变化值\n",
    "s3 = next_status[2] - cur_status[2]    # 目标生命变化值\n",
    "s4 = next_status[3] - cur_status[3]    # 目标架势变化值\n",
    "\n",
    "# 示例 定义得分\n",
    "s1 *=  1    # 正相关\n",
    "s2 *= -1    # 负相关\n",
    "s3 *= -1    # 负相关\n",
    "s4 *=  1    # 正相关\n",
    "\n",
    "reward = s1 + s2 + s3 +s4\n",
    "# ---------- 以上根据 others.py 里定义的函数 get_status 来修改 ----------\n",
    "~~~\n",
    "保存 run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 最后，参数设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 控制背景信息的输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$~~~$$~~~$只狼是3D游戏，背景是动态且复杂的。大量的背景不仅对训练无益，还会导致模型出现严重的欠拟合现象。在这种情况下，智能体连什么是背景什么是人物都分不清，就更别提训练要有什么效果了。  \n",
    "$~~~$$~~~$目前背景减除的各种方法，都是基于静态背景的。比如用摄像头做行人检测，摄像头是固定的，所以背景不变，变的只有行人。就算摄像头会转动，但起码整个背景空间有限。  \n",
    "$~~~$$~~~$那么不做背景减除，换个角度思考，先做运动目标检测，再通过掩码的方式保留运动目标，去除背景。不得不说，这个效果肯定会比什么都不做好，但是问题仍然没有解决。目标检测的结果是矩形的，仍然有部分背景没有去除。  \n",
    "$~~~$$~~~$那人体姿态估计呢，图像什么的都不要了，就留个火柴人，用三维卷积来学习运动轨迹。这个想法非常棒，但是我不确定人体姿态估计能不能检测只狼里面的人，而且破戒僧、狮子猿、怨恨之鬼还有鬼形部身下的马觉得很淦。  \n",
    "$~~~$$~~~$**所以，这部分我也无能为力了**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 目前使用的解决方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化人物状态的时候，需要整个游戏画面，但是给模型训练的时候并不需要那么多，特别是背景，对于训练来说是冗余的。我们需要的只是双方对战的部分图像就够了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个时候，就要设置以下参数\n",
    "~~~python\n",
    "x   = 0    # 左 不小于0，小于 x_w\n",
    "x_w = 0    # 右 不大于图像宽度，例如 800，大于 x\n",
    "y   = 0    # 上 不小于0，小于 y_h\n",
    "y_h = 0    # 下不大于图像高度，例如 450，大于 y\n",
    "~~~\n",
    "由于双方的动作大部分处于中间\n",
    "所以我们可以先暂时作以下设置\n",
    "~~~python\n",
    "x   = 800 // 2 - 100    # 左 不小于0，小于 x_w\n",
    "x_w = 800 // 2 + 100    # 右 不大于图像宽度，例如 800，大于 x\n",
    "y   = 450 // 2 - 100    # 上 不小于0，小于 y_h\n",
    "y_h = 450 // 2 + 100    # 下不大于图像高度，例如 450，大于 y\n",
    "~~~\n",
    "这样就获得了 200x200 战斗部分图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试效果\n",
    "import cv2\n",
    "\n",
    "from game_player.grab_screen import get_game_screen\n",
    "from game_player.others import roi, get_xywh\n",
    "\n",
    "x   = 800 // 2 - 100    # 左 不小于0，小于 x_w\n",
    "x_w = 800 // 2 + 100    # 右 不大于图像宽度，例如 800，大于 x\n",
    "y   = 450 // 2 - 100    # 上 不小于0，小于 y_h\n",
    "y_h = 450 // 2 + 100    # 下不大于图像高度，例如 450，大于 y\n",
    "\n",
    "while True:\n",
    "    screen = get_game_screen()\n",
    "    cv2.imshow('screen', screen)\n",
    "    cv2.imshow('roi', roi(screen, x, x_w, y, y_h))\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不够，就增大范围，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试效果\n",
    "import cv2\n",
    "\n",
    "from game_player.grab_screen import get_game_screen\n",
    "from game_player.others import roi, get_xywh\n",
    "\n",
    "# 400 x 400 战斗部分图像\n",
    "x   = 800 // 2 - 200    # 左 不小于0，小于 x_w\n",
    "x_w = 800 // 2 + 200    # 右 不大于图像宽度，例如 800，大于 x\n",
    "y   = 450 // 2 - 200    # 上 不小于0，小于 y_h\n",
    "y_h = 450 // 2 + 200    # 下不大于图像高度，例如 450，大于 y\n",
    "\n",
    "while True:\n",
    "    screen = get_game_screen()\n",
    "    cv2.imshow('screen', screen)\n",
    "    cv2.imshow('roi', roi(screen, x, x_w, y, y_h))\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 设置输入模型的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "in_depth    = 1     # 三维卷积用的，二维卷积不用管，也不要修改\n",
    "in_height   = 50    # 图像高度，图像缩放用\n",
    "in_width    = 50    # 图像宽度，图像缩放用\n",
    "in_channels = 1     # 颜色通道数量\n",
    "outputs = 4     # 动作数量，智能体能够执行几个动作就写几个，也可以 +1 ，增加其他类\n",
    "lr = 0.001      # 默认学习率是0.001，如果要修改的话，前期可以设置大点，快速收敛，后期设置小一点，提升学习效果\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：in_height, in_width是输入模型的尺寸，最好和战斗部分图像的尺寸同时成比例缩放**\n",
    "e.g. \n",
    "~~~python\n",
    "x   = 800 // 2 - 200\n",
    "x_w = 800 // 2 + 200\n",
    "y   = 450 // 2 - 200\n",
    "y_h = 450 // 2 + 200\n",
    "\n",
    "w = x_w - x    # 结果为 400\n",
    "h = y_h - y    # 结果为 400\n",
    "\n",
    "in_height   = 50\n",
    "in_width    = 50\n",
    "\n",
    "print(h // in_height, w // in_width)    # 结果为 8, 8，代表同时缩放了 8 倍\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 强化学习部分参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "gamma = 0.99    # 奖励衰减，未来对现在的重要程度，设置为 1 代表同等重要，模型更有远瞻性；设置的越小说明越重视当前的决策。\n",
    "replay_memory_size = 10000    # 记忆容量\n",
    "replay_start_size = 500       # 开始经验回放时存储的记忆量，到达最终探索率后才开始\n",
    "batch_size = 16               # 样本抽取数量\n",
    "update_freq = 200                   # 训练评估网络的频率\n",
    "target_network_update_freq = 500    # 更新目标网络的频率\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开 run.py ,修改如下  \n",
    "**修改前**\n",
    "~~~python\n",
    "# -------------------- 一些参数，根据实际情况修改 --------------------\n",
    "\n",
    "x   = 0\n",
    "x_w = 0\n",
    "y   = 0\n",
    "y_h = 0\n",
    "\n",
    "in_depth    = 1\n",
    "in_height   = 50    # 图像高度\n",
    "in_width    = 50    # 图像宽度\n",
    "in_channels = 1     # 颜色通道数量\n",
    "outputs = 4     # 动作数量\n",
    "lr = 0.001      # 学习率\n",
    "\n",
    "gamma = 0.99    # 奖励衰减\n",
    "replay_memory_size = 10000    # 记忆容量\n",
    "replay_start_size = 500       # 开始经验回放时存储的记忆量，到达最终探索率后才开始\n",
    "batch_size = 16               # 样本抽取数量\n",
    "update_freq = 200                   # 训练评估网络的频率\n",
    "target_network_update_freq = 500    # 更新目标网络的频率\n",
    "\n",
    "# -------------------- 一些参数，根据实际情况修改 --------------------\n",
    "~~~\n",
    "**修改后**\n",
    "~~~python\n",
    "# -------------------- 一些参数，根据实际情况修改 --------------------\n",
    "\n",
    "# 400 x 400 战斗部分图像\n",
    "x   = 800 // 2 - 200    # 左 不小于0，小于 x_w\n",
    "x_w = 800 // 2 + 200    # 右 不大于图像宽度，例如 800，大于 x\n",
    "y   = 450 // 2 - 200    # 上 不小于0，小于 y_h\n",
    "y_h = 450 // 2 + 200    # 下不大于图像高度，例如 450，大于 y\n",
    "\n",
    "in_depth    = 1\n",
    "in_height   = 50    # 图像高度\n",
    "in_width    = 50    # 图像宽度\n",
    "in_channels = 1     # 颜色通道数量\n",
    "outputs = 4     # 动作数量\n",
    "lr = 0.001      # 学习率\n",
    "\n",
    "gamma = 0.99    # 奖励衰减\n",
    "replay_memory_size = 10000    # 记忆容量\n",
    "replay_start_size = 500       # 开始经验回放时存储的记忆量，到达最终探索率后才开始\n",
    "batch_size = 16               # 样本抽取数量\n",
    "update_freq = 200                   # 训练评估网络的频率\n",
    "target_network_update_freq = 500    # 更新目标网络的频率\n",
    "\n",
    "# -------------------- 一些参数，根据实际情况修改 --------------------\n",
    "~~~\n",
    "保存 run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_player.run import Agent\n",
    "target = 'Genichiro_Ashina' # 苇名弦一郎\n",
    "# target = 'Inner_Genichiro' # 心中的弦一郎\n",
    "# target = 'True Monk' # 宫内破戒僧\n",
    "# target = 'Isshin,_the_Sword_Saint' # 剑圣一心\n",
    "# target = 'Inner_Isshin' # 心中的一心\n",
    "agent = Agent(\n",
    "    save_memory_path = target + '_memory.json',    # 注释这行就不保存记忆\n",
    "    load_memory_path = target + '_memory.json',    # 注释这行就不加载记忆\n",
    "    save_weights_path = target + '_w.h5',    # 注释这行就不保存模型权重\n",
    "    load_weights_path = target + '_w.h5'     # 注释这行就不加载模型权重\n",
    ")\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_player.run import Agent\n",
    "target = 'Genichiro_Ashina' # 苇名弦一郎\n",
    "# target = 'Inner_Genichiro' # 心中的弦一郎\n",
    "# target = 'True Monk' # 宫内破戒僧\n",
    "# target = 'Isshin,_the_Sword_Saint' # 剑圣一心\n",
    "# target = 'Inner_Isshin' # 心中的一心\n",
    "\n",
    "train = Play_Sekiro_Online(\n",
    "    load_weights_path = target + '_w.h5'\n",
    ")\n",
    "train.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
