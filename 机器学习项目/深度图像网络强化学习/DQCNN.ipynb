{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T13:59:10.972115Z",
     "start_time": "2023-06-20T13:59:04.361591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
       " LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from environment import stock\n",
    "import time\n",
    "import gc\n",
    "from sys import getsizeof\n",
    "\n",
    "tf.config.list_logical_devices()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T13:59:11.019115Z",
     "start_time": "2023-06-20T13:59:11.005116Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_total_reward(rewards): \n",
    "    gc.collect()   \n",
    "    plt.rcParams[\"font.sans-serif\"] = [\"Microsoft YaHei\"]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"回合\", fontsize=14)\n",
    "    plt.ylabel(\"奖励总和\", fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.title(\"训练得分情况\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T13:59:31.002806Z",
     "start_time": "2023-06-20T13:59:11.051118Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on the CPU\n",
    "\n",
    "n_outputs = 5  # == env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Conv2D(\n",
    "            16, 6, activation=\"relu\", padding=\"same\", input_shape=[575, 800, 3]\n",
    "        ),\n",
    "        keras.layers.MaxPool2D(2),  # 池化层，大小2，在每个维度除以2(等于步幅)\n",
    "        keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\"),\n",
    "        # 重复滤波、池化，每池化一次，滤波器数理加倍，因为池化后空间维度除以2，不用担心参数爆炸增长\n",
    "        keras.layers.MaxPooling2D(2),\n",
    "        keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "        keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "        keras.layers.MaxPooling2D(2),\n",
    "        keras.layers.Flatten(),  # 把图像转为一维数组，接上全连接网络\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),  # 每次drop一半神经元防止过拟合\n",
    "        keras.layers.Dense(16, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(n_outputs),  # 输出五个动作价值\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贪婪策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T13:59:31.080725Z",
     "start_time": "2023-06-20T13:59:31.054826Z"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(model, state, epsilon=0):  # epsilon设定贪婪度，越高则越选择Q值高的动作 \n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)  # 在1-epsilon概率下随机选择动作\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis], verbose=0)[\n",
    "            0\n",
    "        ]  # 出来之后应该有两个维度，去掉最外层方便后续\n",
    "        return Q_values.argmax()  # DQN给出的Q值最高的动作索引"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建经验表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T13:59:31.144726Z",
     "start_time": "2023-06-20T13:59:31.130659Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# 重播缓存\n",
    "replay_buffer = deque(maxlen=2000)  #### 长度若干的双端列表，性质类似list但是处理复杂度更低\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 经验采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T13:59:44.005189Z",
     "start_time": "2023-06-20T13:59:43.987679Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    # 0~n的均匀分布，抽取大小为batch_size的随机数组\n",
    "    batch = [replay_buffer[index] for index in indices]  # 按随机序号抽取缓存中的经验\n",
    "    return [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)\n",
    "    ]  # [states, actions, rewards, next_states, dones] 每个元素是一个ndarray数组\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行单步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T13:59:44.921997Z",
     "start_time": "2023-06-20T13:59:44.903986Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_one_step(model, env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(model, state, epsilon)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))  # 经验缓存中增加经验\n",
    "    return next_state, reward, done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T13:59:45.514263Z",
     "start_time": "2023-06-20T13:59:45.488753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "v-2.0. 去掉了历史记录\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "rewards = []\n",
    "best_score = 0\n",
    "batch_size = 48\n",
    "discount_factor = 0.95\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)  # 优化器\n",
    "loss_fn = tf.keras.losses.mean_squared_error  # 损失函数mse\n",
    "\n",
    "env = stock()\n",
    "# model.load_weights(\"DQCNN.h5\") #### 如果已经有模型则读取"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T13:59:46.595328Z",
     "start_time": "2023-06-20T13:59:46.275506Z"
    }
   },
   "outputs": [],
   "source": [
    "target = tf.keras.models.clone_model(model)  # 复制一个新的模型（初始状态一致）\n",
    "target.set_weights(model.get_weights())  # 应用权重\n",
    "\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)  # 提取经验\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "    next_Q_values = model.predict(next_states, verbose=0)  # ≠ target.predict()\n",
    "    best_next_actions = next_Q_values.argmax(axis=1)  # 直接取动作而非Q值\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    max_next_Q_values = (target.predict(next_states, verbose=0) * next_mask).sum(axis=1)\n",
    "    # 计算Q值使用目标模型\n",
    "\n",
    "    runs = 1.0 - dones\n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import muppy, summary\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T14:03:39.678443Z",
     "start_time": "2023-06-20T14:02:19.688811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================0======================\n",
      "replay_buffer占用 7.31 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================1======================\n",
      "replay_buffer占用 7.83 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================2======================\n",
      "replay_buffer占用 8.34 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================3======================\n",
      "replay_buffer占用 8.86 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================4======================\n",
      "replay_buffer占用 9.38 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================5======================\n",
      "replay_buffer占用 9.38 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================6======================\n",
      "replay_buffer占用 9.89 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================7======================\n",
      "replay_buffer占用 10.41 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================8======================\n",
      "replay_buffer占用 10.92 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================9======================\n",
      "replay_buffer占用 11.44 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================10======================\n",
      "replay_buffer占用 11.95 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================11======================\n",
      "replay_buffer占用 12.47 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================12======================\n",
      "replay_buffer占用 12.98 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================13======================\n",
      "replay_buffer占用 13.5 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================14======================\n",
      "replay_buffer占用 14.02 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================15======================\n",
      "replay_buffer占用 14.53 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================16======================\n",
      "replay_buffer占用 15.05 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================17======================\n",
      "replay_buffer占用 15.56 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================18======================\n",
      "replay_buffer占用 16.08 KB内存\n",
      "total_reward占用 0.03 KB内存\n",
      "==================19======================\n",
      "replay_buffer占用 16.59 KB内存\n",
      "total_reward占用 0.03 KB内存\n"
     ]
    }
   ],
   "source": [
    "epi = 20\n",
    "for episode in range(epi):  # m回合\n",
    "    if episode >= 49 and (episode + 1) % 10 == 0:\n",
    "        start_time = time.time()\n",
    "    obs = env.reset(k_num=50)  # 重写初始化状态\n",
    "    total_reward = 0\n",
    "    for step in range(60):  # 执行最多n步\n",
    "        epsilon = max(1 - episode / (epi / 2), 0.01)  # 功率调节，最小是0.01，线性降低\n",
    "        obs, reward, done = play_one_step(model, env, obs, epsilon)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # 应用最佳权重\n",
    "    rewards.append(total_reward)  # 记录本回合最终分数\n",
    "    if (episode + 1) >= 30:\n",
    "        recent_reward = np.sum(np.array(rewards[-30:]) > 0) # 计算近30回合奖励大于0的次数\n",
    "        mean_reward = np.mean(rewards) # 全部平均\n",
    "    else:\n",
    "        recent_reward = mean_reward = -1.0\n",
    "    \n",
    "    if reward >= best_score:\n",
    "        best_weights = model.get_weights()  # 获得本回合的权重\n",
    "        best_score = reward  # 保留该回合最高分\n",
    "        model.save_weights('DQCNN.h5')\n",
    "\n",
    "    if episode > 70:  # 若干个回合之后再训练，目的是先增加经验，但是具体加了多少不确定，因为每回合步骤是随机的\n",
    "        training_step(batch_size)\n",
    "        if (episode + 1 % 50) == 0:  ##### 若干回合修改一次目标模型的权重 #####\n",
    "            target.set_weights(model.get_weights())\n",
    "    \n",
    "    # 时间估计\n",
    "    if (episode + 1) % 30 == 0 and (episode + 1) >= 60:\n",
    "        end_time = time.time()\n",
    "        period_time = end_time - start_time\n",
    "        estimated_left_time = float(\n",
    "            (epi - 1 - episode) * period_time / 1800\n",
    "        )  # 除以30回合除以60分钟，故1800\n",
    "    elif episode <= 59:\n",
    "        estimated_left_time = -1.0\n",
    "\n",
    "    # 报告该回合信息\n",
    "    # print(f\"\\r<{episode + 1}>, 本回合步长: {step + 1}, ε: {epsilon:.3f}, 总奖励: {total_reward:.2f}, 平均奖励: {mean_reward:.2f}, [{recent_reward}/30], 预计剩余: {estimated_left_time:.1f}分钟          \", end=\"\")\n",
    "\n",
    "    ################### 调试区 ######################\n",
    "    print('=================={:d}======================'.format(episode))\n",
    "    '''\n",
    "    # 检测当前进程使用的内存\n",
    "    print('当前内存使用情况：', psutil.Process().memory_info().rss/1024/1024, 'MB')\n",
    "    \n",
    "    # 检测程序堆栈中的所有对象\n",
    "    all_objects = muppy.get_objects()\n",
    "    print('当前堆栈中的对象数量：', len(all_objects))\n",
    "\n",
    "    # 检测每个对象占用的内存，找到内存使用最高的对象\n",
    "    sum1 = summary.summarize(all_objects)\n",
    "    summary.print_(sum1)\n",
    "\n",
    "    print()\n",
    "    '''\n",
    "    \n",
    "    del reward, total_reward, obs # 回收reward和total_reward[测试]\n",
    "    gc.collect()\n",
    "    \n",
    "    ################### 调试区 #######################\n",
    "    \n",
    "model.set_weights(best_weights)  # 最佳权重赋值到模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('DQCNN_end_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-20T14:04:17.948719Z",
     "start_time": "2023-06-20T14:04:17.551472Z"
    }
   },
   "outputs": [],
   "source": [
    "show_total_reward(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
